{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gen√≠s Lloses 218873 <br>\n",
    "Bernat Treserres 217387 <br>\n",
    "Carla Souto 218871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path = 'dataset_tweets_WHO.txt'\n",
    "with open(docs_path) as fp:\n",
    "    tweets = json.loads(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_symbols_and_links(text):\n",
    "    text = re.sub('https://.*', '', text) # to remove the links\n",
    "    text = re.sub('http://.*', '', text)\n",
    "    for ch in ['&',':','.',',',';','‚Ä¶','-','!','?','¬ø','amp','rt','\"',\"'\"]:\n",
    "        if ch in text:\n",
    "            text = text.replace(ch, '')\n",
    "    return text\n",
    "\n",
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line= line.lower() ## Transform in lowercase\n",
    "    line= remove_emoji(line) ## remove emojis\n",
    "    line= remove_symbols_and_links(line) ## remove symbols and links\n",
    "    line= line.split() ## Tokenize the text to get a list of terms\n",
    "    hashtags= [l.replace('#', '') for l in line if '#' in l] # we separate the hashtags\n",
    "    line= [l for l in line if l not in stop_words and l.replace('#', '') not in hashtags and len(l)>1] ## eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line= [stemmer.stem(l) for l in line] ## perform stemming for the keywords\n",
    "    hashtags= [stemmer.stem(l) for l in hashtags] ## perform stemming for the hashtags\n",
    "    ## END CODE\n",
    "    return line, hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_keywords = {}\n",
    "\n",
    "for key in tweets.keys():\n",
    "    tweets_keywords[int(key)] = build_terms(tweets[key]['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The right to health means the right to control one‚Äôs health and body ‚Äì including the sexual and reproductive health rights of women and girls ‚Äì without interference https://t.co/0xs5xwkoTg \n",
      "\n",
      "#HealthForAll #StandUp4HumanRights https://t.co/qfh5PQ8S2D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['right',\n",
       "  'health',\n",
       "  'mean',\n",
       "  'right',\n",
       "  'control',\n",
       "  'one‚Äô',\n",
       "  'health',\n",
       "  'bodi',\n",
       "  'includ',\n",
       "  'sexual',\n",
       "  'reproduct',\n",
       "  'health',\n",
       "  'right',\n",
       "  'women',\n",
       "  'girl',\n",
       "  'without',\n",
       "  'interfer'],\n",
       " ['healthforal', 'standup4humanright'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#An example of how the list is stored (in this case, for the last word of the dictionary)\n",
    "\n",
    "print(tweets['2398']['full_text']) # the full text of the tweet\n",
    "tweets_keywords[2398] # the list of keywords and hastags of that tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(tweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    keywords_index = defaultdict(list)\n",
    "    hashtags_index = defaultdict(list)\n",
    "\n",
    "    for tweet_id in tweets.keys():  # Remember, lines contain all documents\n",
    "        tweet = tweets[tweet_id][\"full_text\"]\n",
    "        keywords, hashtags = build_terms(tweet)\n",
    "         \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { ‚Äòterm1‚Äô: [current_doc, [list of positions]], ...,‚Äòterm_n‚Äô: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { ‚Äòweb‚Äô: [1, [0]], ‚Äòretrieval‚Äô: [1, [1,4]], ‚Äòinformation‚Äô: [1, [2]]}\n",
    "\n",
    "        ## the term ‚Äòweb‚Äô appears in document 1 in positions 0, \n",
    "        ## the term ‚Äòretrieval‚Äô appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_tweet_keyword_index = {}\n",
    "        current_tweet_hashtag_index = {}\n",
    "\n",
    "        for position, keyword in enumerate(keywords): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                \n",
    "                current_tweet_keyword_index[keyword][1].append(position)  \n",
    "                \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_tweet_keyword_index[keyword]=[tweet_id, [position]] #'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for keyword, info in current_tweet_keyword_index.items():\n",
    "            keywords_index[keyword].append(info)\n",
    "        \n",
    "        \n",
    "        for position, hashtag in enumerate(hashtags): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                \n",
    "                current_tweet_hashtag_index[hashtag][1].append(position) \n",
    "                \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_tweet_hashtag_index[hashtag]=[tweet_id, [position]] #'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for hashtag, info in current_tweet_hashtag_index.items():\n",
    "            hashtags_index[keyword].append(info)\n",
    "        \n",
    "                    \n",
    "    return keywords_index, hashtags_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_info = {}\n",
    "\n",
    "for tweet_id in tweets.keys():\n",
    "    tweet_info = {}\n",
    "    tweet_info[\"Tweet\"] = tweets[tweet_id][\"full_text\"]\n",
    "    tweet_info[\"Username\"] = tweets[tweet_id][\"user\"][\"screen_name\"]\n",
    "    tweet_info[\"Date\"] = tweets[tweet_id][\"created_at\"]\n",
    "    tweet_info[\"Hashtags\"] = tweets[tweet_id][\"entities\"][\"hashtags\"]\n",
    "    tweet_info[\"Likes\"] = tweets[tweet_id][\"favorite_count\"]\n",
    "    tweet_info[\"Retweets\"] = tweets[tweet_id][\"retweet_count\"]\n",
    "    try:\n",
    "        tweet_info[\"Url\"] = tweets[tweet_id][\"entities\"][\"urls\"][0][\"url\"]\n",
    "    except:\n",
    "        tweet_info[\"Url\"] = \"No Url :(\"\n",
    "    \n",
    "    tweets_info[int(tweet_id)] = tweet_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREC Q EN COMPTES DE PASSARLI TWEETS PODEM PASSAR EL NUM DE TWEETS PERO BUENO\n",
    "\n",
    "\n",
    "def create_index_tfidf(num_tweets, index):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    numDocuments -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\" \n",
    "        \n",
    "    tf=defaultdict(list) #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df=defaultdict(int)         #document frequencies of terms in the corpus\n",
    "    idf=defaultdict(float)\n",
    "        \n",
    "    #normalize term frequencies\n",
    "    # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "    # norm is the same for all terms of a document.\n",
    "    norm=0\n",
    "    for term, values in index.items(): \n",
    "        # posting is a list containing doc_id and the list of positions for current term in current document: \n",
    "        # posting ==> [currentdoc, [list of positions]] \n",
    "        # you can use it to inferr the frequency of current term.\n",
    "        for tweet in values:\n",
    "            norm+=len(tweet[1])**2 # numbers of appearances of a term \n",
    "    norm=math.sqrt(norm)\n",
    "\n",
    "\n",
    "    #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "    for term, values in index.items():     \n",
    "        # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "        for tweet in values:\n",
    "            tf[term].append(np.round(len(tweet[1])/norm,4))  ## SEE formula (1) above\n",
    "        #increment the document frequency of current term (number of documents containing the current term)\n",
    "        df[term]= len(index[term])  # increment df for current term\n",
    "\n",
    "\n",
    "    # Compute idf following the formula (3) above. HINT: use np.log\n",
    "    for term in df:\n",
    "        idf[term] = np.round(np.log(float(num_tweets/df[term])),4)\n",
    "            \n",
    "    return tf, df, idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 5.88 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_tweets = len(tweets)\n",
    "\n",
    "keywords_index, hashtags_index = create_index(tweets)\n",
    "\n",
    "tf_k, df_k, idf_k = create_index_tfidf(len(tweets), keywords_index)\n",
    "tf_h, df_h, idf_h = create_index_tfidf(len(tweets), hashtags_index)\n",
    "\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_tweets(terms, tweets, index, idf, tf):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    tweet_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex]= query_terms_count[term]/query_norm * idf[term]\n",
    "        \n",
    "        # Generate doc_vectors for matching docs\n",
    "        for tweet_index, (tweet, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if tweet in tweets:\n",
    "                tweet_vectors[tweet][termIndex] = tf[term][tweet_index] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queryVector and each tweetVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "    \n",
    "    tweet_scores=[[np.dot(curTweetVec, query_vector), tweet] for tweet, curTweetVec in tweet_vectors.items() ]\n",
    "    tweet_scores.sort(reverse=True)\n",
    "    result_tweets = [x[1] for x in tweet_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_tweets) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return result_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)[0]\n",
    "    tweets = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_tweets the ids of the tweets that contain \"term\"                        \n",
    "            term_tweets = []\n",
    "            for posting in index[term]:\n",
    "                term_tweets.append(posting[0])\n",
    "            \n",
    "            # tweets = tweets Union term_tweets\n",
    "            tweets = tweets.union(term_tweets)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    tweets = list(tweets)\n",
    "    ranked_tweets = rank_tweets(query, tweets, index, idf_k, tf_k)\n",
    "    return ranked_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "covid vaccine\n",
      "\n",
      "======================\n",
      "Top 10 results out of 427 for the searched query:\n",
      "======================\n",
      "\n",
      "tweet_id= 61\n",
      "tweet: RT @WHOPhilippines: Vaccines can‚Äôt stop #COVID19 alone, but by doing it all we can help protect ourselves and our loved ones against COVID-‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 200\n",
      "tweet: üÜï WHO clinical case definition for post #COVID19 condition, also called 'long COVID' https://t.co/WoiLcwsgJX https://t.co/Z0olrHlWPC\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1561\n",
      "tweet: If you have recovered from #COVID19 but are still experiencing certain symptoms you could have post COVID-19 condition or \"long COVID\". What are these symptoms? How long do they last and are there any treatment options? Dr @diazjv explains in #ScienceIn5 ‚¨áÔ∏è https://t.co/vtDiBhZsJE\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1110\n",
      "tweet: To facilitate effective vaccination delivery for persons with disabilities, health providers should\n",
      "\n",
      "‚úÖ provide accessible #COVID19 vaccine info &amp; vaccination processes\n",
      "‚úÖ Ensure accessibility to vaccination sites\n",
      "\n",
      "üëâhttps://t.co/nHE5btUazT #VaccinEquity https://t.co/ZvpsqEqsph\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 904\n",
      "tweet: #COVID19 variants &amp; vaccines:\n",
      "\n",
      "‚úÖ COVID-19 vaccines provide strong protection against serious illness &amp; death\n",
      "‚úÖ Get all necessary doses to develop maximum protection\n",
      "‚úÖ Continue practicing all the protective behaviours even after vaccination to stop COVID-19 variants\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 510\n",
      "tweet: Donations to COVAX are an important source of #COVID19 vaccine supply; however, these should complement rather than replace vaccine procurement by COVAX given the high transaction burden &amp; costs in managing these donations.: The Independent Allocation Vaccine Group for COVAX https://t.co/8duByptSTj\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 509\n",
      "tweet: Programmes put in place to increase confidence in #COVID19 vaccines &amp; address vaccination hesitancy must be tailored to local contexts; engagement of local communities &amp; civil society is critical to ensuring their effectiveness: The Independent Allocation Vaccine Group for COVAX https://t.co/bMhbzqu2dX\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 2001\n",
      "tweet: Through the¬†Immunization Agenda 2030, WHO &amp; partners aim to\n",
      "\n",
      "üéØ Achieve 90% coverage for essential vaccines given in childhood &amp; adolescence\n",
      "üéØ Halve the number of children who completely miss out on vaccination\n",
      "üéØ Introduce 500 new vaccines in low &amp; middle-income countries https://t.co/unN8nhC0sv\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1968\n",
      "tweet: All¬†#COVID19 vaccines¬†approved by WHO have been through clinical trials to test their quality, safety &amp; efficacy. After approval, they are monitored for ongoing safety &amp; effectiveness.\n",
      "Here's the difference between vaccine efficacy &amp; vaccine effectiveness: https://t.co/rZKe9dr2D6 https://t.co/5us1nLqpzm\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 164\n",
      "tweet: #VaccinEquity is required to vaccinate 70% of the global population. There will be sufficient vaccine from a supply perspective to achieve the global #COVID19 vaccination targets, what we need is equitable distribution of those doses around the üåéüåçüåè\n",
      "\n",
      "üëâ https://t.co/TIshOPIbiO https://t.co/IB2AT5BvcA\n",
      "\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "relevance_dict = {}\n",
    "\n",
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_tweets = search_tf_idf(query, keywords_index)\n",
    "top = 10\n",
    "\n",
    "relevance_dict[\"q1\"] = {\"tweets_id\": ranked_tweets[:top], \"relevance\": [0,0,1,1,0,0,1,1,0,0]} \n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n======================\\n\".format(top, len(ranked_tweets)))\n",
    "for t_id in ranked_tweets[:top]:\n",
    "    print(\"tweet_id= {}\\ntweet: {}\\n\".format(t_id, tweets[str(t_id)]['full_text']))\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "hospital doctor\n",
      "\n",
      "======================\n",
      "Top 10 results out of 59 for the searched query:\n",
      "======================\n",
      "\n",
      "tweet_id= 823\n",
      "tweet: RT @WHOWPRO: Gatsengel of #Mongolia is an intensive care doctor who lost his grandmother to #COVID19. Yet, loyal to his oath and motivated‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 663\n",
      "tweet: A brain drain in #Lebanon is occurring at alarming speed. Almost 40% of skilled medical doctors and almost 30% of registered nurses have already left the country either permanently or temporarily. https://t.co/xvrdG3Ir0g https://t.co/JuC5teKsOJ\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 654\n",
      "tweet: RT @DrTedros: Very touched with meeting in person Afghan evacuees in üá∂üá¶. Some are journalists, doctors &amp; artists. I met their children too.‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 569\n",
      "tweet: @DrTedros @WHOEMRO \"There's a serious shortage of supplies, medical equipment, fuel and electricity.\n",
      "2000 medical doctors and 1500 registered nurses have left, and 600 private pharmacies have closed\"-@DrTedros #Lebanon \n",
      "\n",
      "https://t.co/ZfIajnvDiU\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 2374\n",
      "tweet: @DrTedros \"Emergency Medical Teams, or EMTs, are groups of health professionals, including doctors, nurses, paramedics, support workers and logisticians, who provide care for patients affected by an emergency\"-@DrTedros \n",
      "\n",
      "https://t.co/GSkRV0IKlj\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 118\n",
      "tweet: A team of #PalliativeCare includes\n",
      " \n",
      "üë• doctors\n",
      "üë• nurses\n",
      "üë• support workers\n",
      "üë• pharmacists\n",
      "üë• social workers\n",
      "üë• physiotherapists\n",
      "üë• volunteers\n",
      " \n",
      "working together to support patients &amp; families who deal with life-threatening illnesses &amp; suffering.\n",
      " \n",
      "üëâhttps://t.co/5fMFJuDYYr https://t.co/WMDuMIvzX5\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1963\n",
      "tweet: üÜï report confirms that #HIV infection is a significant independent risk factor for both severe/critical #COVID19 presentation at hospital admission &amp; in-hospital mortality. 23.1% of all people living with HIV who were hospitalized with COVID-19, died.\n",
      "\n",
      "üëâ https://t.co/UBd0OyFKnd https://t.co/UHLo9RPa1g\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 136\n",
      "tweet: Centralized mental hospitals &amp; institutional inpatient care still receive more funds than services provided in general hospitals &amp; primary health-care centres in many countries.\n",
      "The transfer of care to the community needs to be accelerated https://t.co/J0uKvKWZ46 \n",
      "\n",
      "#MentalHealth https://t.co/uTN6DN7yO7\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 861\n",
      "tweet: @DrTedros \"While #COVID19 cases in some of these countries are increasing among unvaccinated people, hospitalizations and deaths have remained relatively low, thanks to vaccines and earlier clinical care\"-@DrTedros\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 794\n",
      "tweet: Anyone with an infection can develop #sepsis, but some people are more at risk than others:\n",
      "\n",
      "üî∏Pregnant women\n",
      "üî∏Newborns\n",
      "üî∏The elderly\n",
      "üî∏People with chronic diseases or immunosuppression\n",
      "üî∏Hospitalized patients\n",
      "\n",
      "https://t.co/ejYtk5W1Nn https://t.co/8LTGOnABkl\n",
      "\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_tweets = search_tf_idf(query, keywords_index)\n",
    "top = 10\n",
    "\n",
    "relevance_dict[\"q2\"] = {\"tweets_id\": ranked_tweets[:top], \"relevance\": [1,1,0,1,0,0,0,1,0,1,0]} \n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n======================\\n\".format(top, len(ranked_tweets)))\n",
    "for t_id in ranked_tweets[:top]:\n",
    "    print(\"tweet_id= {}\\ntweet: {}\\n\".format(t_id, tweets[str(t_id)]['full_text']))\n",
    "    print(\"--------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "fast food\n",
      "\n",
      "======================\n",
      "Top 10 results out of 57 for the searched query:\n",
      "======================\n",
      "\n",
      "tweet_id= 2026\n",
      "tweet: Food systems &amp; health are connected:\n",
      "¬†\n",
      "üîÄ from those who work üë©üèΩ‚Äçüåæ in our food systems\n",
      "üîÄ the practices used to grow, raise, transport package &amp; dispose of our food\n",
      "üîÄ to the quality, quantity &amp; safety of the food we eat\n",
      "¬†\n",
      "More info üëâhttps://t.co/uCqRGjlf3P https://t.co/73YoAlI87v\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 883\n",
      "tweet: To keep food safe at home, follow WHO 5 Keys to Safer Food:¬†\n",
      "1Ô∏è‚É£ Keep clean¬†\n",
      "2Ô∏è‚É£ Separate raw and cooked¬†\n",
      "3Ô∏è‚É£ Cook thoroughly¬†\n",
      "4Ô∏è‚É£ Keep food at safe temperature¬†\n",
      "5Ô∏è‚É£ Use safe water and raw materials\n",
      "\n",
      "#SafeFood https://t.co/ppX7O2Jpk7\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 955\n",
      "tweet: RT @DrTedros: Viruses move fast, but data can move even faster. With the right information, countries &amp; communities can stay ahead of emerg‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 789\n",
      "tweet: Act fast. Seek medical care if you or someone you know has an infection that is not getting better, or if they have any signs of #sepsis.\n",
      "üî∏Fever or shivering\n",
      "üî∏Confusion\n",
      "üî∏Shortness of breath \n",
      "üî∏Rash that does not fade when pressed\n",
      "üî∏Extreme discomfort\n",
      "üî∏Clammy or sweaty skin https://t.co/ZCgfONXZlJ\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 2119\n",
      "tweet: @DrTedros \"However, compounded by fast moving variants and shocking inequity in vaccination, far too many countries in every region of the world are seeing sharp spikes in #COVID19 cases and hospitalisation\"-@DrTedros\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1943\n",
      "tweet: Responding first and fast is key to saving lives on the front line. To help #healthworkers acquire the skills needed to manage all kinds of health emergencies, WHO is launching the üÜï #Ready4Response curriculum. \n",
      "\n",
      "Train now on #OpenWHO:\n",
      "üëâ https://t.co/hBFFOF0xKL https://t.co/9lRPKOSm5H\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1398\n",
      "tweet: @DrTedros \"Whether we reach 300 million [#COVID19 cases], &amp; how fast we get there, depends on all of us. At the current trajectory, we could pass 300 million reported cases early next year. But we can change that. We‚Äôre all #InThisTogether, but the üåç is not acting like it\"-@DrTedros\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1150\n",
      "tweet: \"WHO and our partners are doing everything we can to find ways of scaling up production as much as possible, as fast as possible.\"-@DrTedros at #RC71AFRO\n",
      "https://t.co/GUtXnyTd2A\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1002\n",
      "tweet: @DrTedros \"Viruses move fast, but data can move even faster. \n",
      "\n",
      "With the right information, countries and communities can stay ahead of emerging risks, and save lives\"-@DrTedros\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 888\n",
      "tweet: More than 600 million people fall ill every year from eating food contaminated with bacteria, viruses, parasites, toxins or chemicals. \n",
      "Everyone has a role to play to keep food safe https://t.co/xPN75h2GdR https://t.co/ozDeAbT8kM\n",
      "\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_tweets = search_tf_idf(query, keywords_index)\n",
    "top = 10\n",
    "\n",
    "relevance_dict[\"q3\"] = {\"tweets_id\": ranked_tweets[:top], \"relevance\": [0,0,0,0,0,0,0,0,0,0]} \n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n======================\\n\".format(top, len(ranked_tweets)))\n",
    "for t_id in ranked_tweets[:top]:\n",
    "    print(\"tweet_id= {}\\ntweet: {}\\n\".format(t_id, tweets[str(t_id)]['full_text']))\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "covid symptoms\n",
      "\n",
      "======================\n",
      "Top 10 results out of 20 for the searched query:\n",
      "======================\n",
      "\n",
      "tweet_id= 1561\n",
      "tweet: If you have recovered from #COVID19 but are still experiencing certain symptoms you could have post COVID-19 condition or \"long COVID\". What are these symptoms? How long do they last and are there any treatment options? Dr @diazjv explains in #ScienceIn5 ‚¨áÔ∏è https://t.co/vtDiBhZsJE\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 700\n",
      "tweet: If you are pregnant &amp; #COVID19 positive:\n",
      " \n",
      "‚è≥ Seek medical care early\n",
      "üìû Ask your health worker for advice on managing symptoms\n",
      "üè• Know where to go if your symptoms worsen\n",
      "üóìÔ∏è Isolate yourself https://t.co/8Yq1BrmJ9l\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 311\n",
      "tweet: Today is #WorldHeartDay ‚ô•Ô∏è\n",
      " \n",
      "Know the symptoms of heart attack and #stroke\n",
      " \n",
      "If you are experiencing any of these symptoms, seek medical care immediately.\n",
      "‚¨áÔ∏è https://t.co/u1K2S9kckx\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 199\n",
      "tweet: People with post #COVID19 condition can experience lingering symptoms for months after recovery.\n",
      "The symptoms may begin right after being sick with COVID-19 or later, and may come and go https://t.co/WoiLcwsgJX https://t.co/JrABQL2EFx\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 172\n",
      "tweet: @DrTedros \"Some patients have reported symptoms of post #COVID19 condition for much longer than 2 months.\n",
      "\n",
      "Common symptoms include fatigue, shortness of breath, cognitive dysfunction, and others that have an impact on everyday functioning\"-@DrTedros\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 61\n",
      "tweet: RT @WHOPhilippines: Vaccines can‚Äôt stop #COVID19 alone, but by doing it all we can help protect ourselves and our loved ones against COVID-‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 200\n",
      "tweet: üÜï WHO clinical case definition for post #COVID19 condition, also called 'long COVID' https://t.co/WoiLcwsgJX https://t.co/Z0olrHlWPC\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 930\n",
      "tweet: Most patients with #COVID19 will only get mild or moderate disease &amp; will not need supplemental oxygen.\n",
      "It is important for them to be isolated from others, whether in a health facility or at home, but their symptoms can be managed with over-the-counter medicines like paracetamol https://t.co/BE9tp9PuME\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 926\n",
      "tweet: ‚ö†Ô∏è If the blood oxygen level falls to the range of 90-94%, it is a sign that you need medical attention. You should reach your health care provider for further guidance.\n",
      "‚ö†Ô∏è If the blood oxygen level falls below 90% or if you have serious symptoms, seek immediate medical attention https://t.co/GtlBHm28pO https://t.co/RmE5dNqupo\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 917\n",
      "tweet: Did you know you can have a Sexually Transmitted Infection without showing any symptoms? \n",
      "It‚Äôs worth getting tested. \n",
      " \n",
      "STI kits allow people to collect their own samples for testing, even if they are not able to get to a clinic\n",
      "\n",
      "https://t.co/9EaojNahXz https://t.co/bsbZW6oC9e\n",
      "\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_tweets = search_tf_idf(query, keywords_index)\n",
    "top = 10\n",
    "\n",
    "relevance_dict[\"q4\"] = {\"tweets_id\": ranked_tweets[:top], \"relevance\": [0,0,1,0,0,1,1,0,1,1]} \n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n======================\\n\".format(top, len(ranked_tweets)))\n",
    "for t_id in ranked_tweets[:top]:\n",
    "    print(\"tweet_id= {}\\ntweet: {}\\n\".format(t_id, tweets[str(t_id)]['full_text']))\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "sport injuries\n",
      "\n",
      "======================\n",
      "Top 10 results out of 21 for the searched query:\n",
      "======================\n",
      "\n",
      "tweet_id= 1909\n",
      "tweet: @Olympics @DrTedros @Tokyo2020 \"Sport and all forms of physical activity are essential for good health. By its very nature, sport is about participation, bringing individuals, communities and countries together, and bridging cultural, ethnic and national divides\"-@DrTedros #Tokyo2020 #AGoal4All\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 699\n",
      "tweet: üÜï WHO/@ILO study reveals that almost 2‚É£ million people were killed by work-related diseases &amp; injuries in 2016.\n",
      "\n",
      "Find out more in the first WHO/ILO Joint Estimates of the Work-related Burden of Disease and Injury üëâhttps://t.co/mEXoBqYt5g #WorkersHealth https://t.co/fV8nJWkNb2\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1908\n",
      "tweet: @Olympics @DrTedros @Tokyo2020 \"Sport promotes tolerance and respect, &amp; empowers women &amp; young people.\n",
      "...\n",
      "I‚Äôm delighted to announce that tomorrow I will sign a new Memorandum of Understanding with Mr Andrew Parsons, the President of the International @Paralympics Committee\"-@DrTedros #Tokyo2020 #AGoal4All\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1883\n",
      "tweet: @Olympics @DrTedros @Tokyo2020 @Paralympics @g20org @ACTAccelerator \"Beyond competition, medals &amp; records, the #Tokyo2020 Games bring the nations of the üåç together in celebration of sport, health, excellence, friendship &amp; respect. But ultimately, they are a celebration of something that our üåç needs now, more than ever: ùê°ùê®ùê©ùêû\"-@DrTedros\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1857\n",
      "tweet: WHO &amp; @Paralympics join forces to promote diversity &amp; equity through global initiatives promoting health and sport for everybody, everywhere.\n",
      "\n",
      "More info üëâhttps://t.co/YiKR6UHx3Z https://t.co/anE6b5OSxL\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1854\n",
      "tweet: Through this new agreement, WHO &amp; @Paralympics will collaborate to\n",
      "\n",
      "‚úÖ improve access to quality rehabilitation &amp; assistive technology\n",
      "‚úÖ mitigate inequalities &amp; promote equal opportunities &amp; participation in sports for persons with disabilities.\n",
      "\n",
      "üëâhttps://t.co/YiKR6UHx3Z https://t.co/ihATTfPpdL\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1851\n",
      "tweet: About 15% of the üåê‚Äôs population live with a disability.\n",
      "\n",
      "This partnership will provide a platform for more persons with disabilities to participate in sport &amp; ensure they have the care &amp; technologies they need to fulfil their potential.\n",
      "\n",
      "Read more üëâhttps://t.co/YiKR6UHx3Z https://t.co/KxA5PZP8jo\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1826\n",
      "tweet: RT @DrTedros: The #Olympics bring the üåç together in celebration of:\n",
      "-sport\n",
      "-health\n",
      "-excellence\n",
      "-teamwork\n",
      "-respect\n",
      "-hope!\n",
      "\n",
      "I wish all the at‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1106\n",
      "tweet: Through this agreement, WHO &amp; @Paralympics will collaborate to\n",
      "\n",
      "‚úÖ improve access to quality rehabilitation &amp; assistive technology\n",
      "‚úÖ mitigate inequalities &amp; promote equal opportunities &amp; participation in sports for people with disabilities.\n",
      "\n",
      "üëâhttps://t.co/YiKR6UHx3Z https://t.co/ZQQg6YKswL\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1105\n",
      "tweet: The partnership between WHO &amp; @Paralympics will provide a platform for more people with disabilities to participate in sport &amp; ensure they have the care &amp; technologies they need to fulfil their potential.\n",
      "\n",
      "Read more üëâhttps://t.co/YiKR6UHx3Z #Paralympics https://t.co/rueIuDX0rO\n",
      "\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_tweets = search_tf_idf(query, keywords_index)\n",
    "top = 10\n",
    "\n",
    "relevance_dict[\"q5\"] = {\"tweets_id\": ranked_tweets[:top], \"relevance\": [0,0,1,1,0,0,1,1,0,0]} \n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n======================\\n\".format(top, len(ranked_tweets)))\n",
    "for t_id in ranked_tweets[:top]:\n",
    "    print(\"tweet_id= {}\\ntweet: {}\\n\".format(t_id, tweets[str(t_id)]['full_text']))\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q1</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q1</td>\n",
       "      <td>1561</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q1</td>\n",
       "      <td>1110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q1</td>\n",
       "      <td>904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>q1</td>\n",
       "      <td>510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>q1</td>\n",
       "      <td>509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>q1</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>q1</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>q1</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>q2</td>\n",
       "      <td>823</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>q2</td>\n",
       "      <td>663</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>q2</td>\n",
       "      <td>654</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>q2</td>\n",
       "      <td>569</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>q2</td>\n",
       "      <td>2374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>q2</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>q2</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>q2</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>q2</td>\n",
       "      <td>861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>q2</td>\n",
       "      <td>794</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>q3</td>\n",
       "      <td>2026</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>q3</td>\n",
       "      <td>883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>q3</td>\n",
       "      <td>955</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>q3</td>\n",
       "      <td>789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>q3</td>\n",
       "      <td>2119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>q3</td>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>q3</td>\n",
       "      <td>1398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>q3</td>\n",
       "      <td>1150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>q3</td>\n",
       "      <td>1002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>q3</td>\n",
       "      <td>888</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>q4</td>\n",
       "      <td>1561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>q4</td>\n",
       "      <td>700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>q4</td>\n",
       "      <td>311</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>q4</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>q4</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>q4</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>q4</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>q4</td>\n",
       "      <td>930</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>q4</td>\n",
       "      <td>926</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>q4</td>\n",
       "      <td>917</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>q5</td>\n",
       "      <td>1909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>q5</td>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>q5</td>\n",
       "      <td>1908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>q5</td>\n",
       "      <td>1883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>q5</td>\n",
       "      <td>1857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>q5</td>\n",
       "      <td>1854</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>q5</td>\n",
       "      <td>1851</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>q5</td>\n",
       "      <td>1826</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>q5</td>\n",
       "      <td>1106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>q5</td>\n",
       "      <td>1105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q_id tweet_id  relevant\n",
       "0    q1       61         0\n",
       "1    q1      200         0\n",
       "2    q1     1561         1\n",
       "3    q1     1110         1\n",
       "4    q1      904         0\n",
       "5    q1      510         0\n",
       "6    q1      509         1\n",
       "7    q1     2001         1\n",
       "8    q1     1968         0\n",
       "9    q1      164         0\n",
       "10   q2      823         1\n",
       "11   q2      663         1\n",
       "12   q2      654         0\n",
       "13   q2      569         1\n",
       "14   q2     2374         0\n",
       "15   q2      118         0\n",
       "16   q2     1963         0\n",
       "17   q2      136         1\n",
       "18   q2      861         0\n",
       "19   q2      794         1\n",
       "20   q3     2026         0\n",
       "21   q3      883         0\n",
       "22   q3      955         0\n",
       "23   q3      789         0\n",
       "24   q3     2119         0\n",
       "25   q3     1943         0\n",
       "26   q3     1398         0\n",
       "27   q3     1150         0\n",
       "28   q3     1002         0\n",
       "29   q3      888         0\n",
       "30   q4     1561         0\n",
       "31   q4      700         0\n",
       "32   q4      311         1\n",
       "33   q4      199         0\n",
       "34   q4      172         0\n",
       "35   q4       61         1\n",
       "36   q4      200         1\n",
       "37   q4      930         0\n",
       "38   q4      926         1\n",
       "39   q4      917         1\n",
       "40   q5     1909         0\n",
       "41   q5      699         0\n",
       "42   q5     1908         1\n",
       "43   q5     1883         1\n",
       "44   q5     1857         0\n",
       "45   q5     1854         0\n",
       "46   q5     1851         1\n",
       "47   q5     1826         1\n",
       "48   q5     1106         0\n",
       "49   q5     1105         0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(columns = ['q_id', 'tweet_id', \"relevant\"])\n",
    "\n",
    "queries = []\n",
    "tweets_ids = []\n",
    "relevances = []\n",
    "\n",
    "for key in relevance_dict.keys():\n",
    "    for i in range(10):\n",
    "        queries.append(key)\n",
    "        tweets_ids.append(relevance_dict[key][\"tweets_id\"][i])\n",
    "        relevances.append(relevance_dict[key][\"relevance\"][i])\n",
    "\n",
    "data['q_id'] = queries\n",
    "data['tweet_id'] = tweets_ids\n",
    "data['relevant'] = relevances\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, k=10):\n",
    "    '''    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    precision @k : float\n",
    "    \n",
    "    '''    \n",
    "    relevant = y_true[:k]\n",
    "    \n",
    "    cont = 0\n",
    "    for rel in relevant:\n",
    "        if rel == 1:\n",
    "            cont += 1\n",
    "    \n",
    "    return cont/k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision at query q1: 0.42857142857142855\n",
      "Precision at query q2: 0.42857142857142855\n",
      "Precision at query q3: 0.0\n",
      "Precision at query q4: 0.42857142857142855\n",
      "Precision at query q5: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "k=7\n",
    "\n",
    "prec_at_q_1 = precision_at_k(data[data[\"q_id\"]==\"q1\"][\"relevant\"], k)\n",
    "prec_at_q_2 = precision_at_k(data[data[\"q_id\"]==\"q2\"][\"relevant\"], k)\n",
    "prec_at_q_3 = precision_at_k(data[data[\"q_id\"]==\"q3\"][\"relevant\"], k)\n",
    "prec_at_q_4 = precision_at_k(data[data[\"q_id\"]==\"q4\"][\"relevant\"], k)\n",
    "prec_at_q_5 = precision_at_k(data[data[\"q_id\"]==\"q5\"][\"relevant\"], k)\n",
    "\n",
    "\n",
    "print(\"Precision at query {}: {}\".format(\"q1\",prec_at_q_1))\n",
    "print(\"Precision at query {}: {}\".format(\"q2\",prec_at_q_2))\n",
    "print(\"Precision at query {}: {}\".format(\"q3\",prec_at_q_3))\n",
    "print(\"Precision at query {}: {}\".format(\"q4\",prec_at_q_4))\n",
    "print(\"Precision at query {}: {}\".format(\"q5\",prec_at_q_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precision_at_k(y_true, k=10):\n",
    "    \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    average precision @k : float\n",
    "    '''\n",
    "    \n",
    "    gtp = np.sum(y_true)           \n",
    "\n",
    "    ## if all docs are not relevant\n",
    "    if gtp==0:\n",
    "        return 0\n",
    "    \n",
    "    n_relevant_at_i = 0\n",
    "    prec_at_i = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 1:\n",
    "            prec_at_i += precision_at_k(y_true, i+1)\n",
    "            \n",
    "    return prec_at_i/gtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision at query q1: 0.44047619047619047\n",
      "Average precision at query q2: 0.75\n",
      "Average precision at query q3: 0\n",
      "Average precision at query q4: 0.40793650793650793\n",
      "Average precision at query q5: 0.44047619047619047\n"
     ]
    }
   ],
   "source": [
    "k=7\n",
    "\n",
    "avg_prec_at_q_1 = avg_precision_at_k(list(data[data[\"q_id\"]==\"q1\"][\"relevant\"]), k)\n",
    "avg_prec_at_q_2 = avg_precision_at_k(list(data[data[\"q_id\"]==\"q2\"][\"relevant\"]), k)\n",
    "avg_prec_at_q_3 = avg_precision_at_k(list(data[data[\"q_id\"]==\"q3\"][\"relevant\"]), k)\n",
    "avg_prec_at_q_4 = avg_precision_at_k(list(data[data[\"q_id\"]==\"q4\"][\"relevant\"]), k)\n",
    "avg_prec_at_q_5 = avg_precision_at_k(list(data[data[\"q_id\"]==\"q5\"][\"relevant\"]), k)\n",
    "\n",
    "\n",
    "print(\"Average precision at query {}: {}\".format(\"q1\",avg_prec_at_q_1))\n",
    "print(\"Average precision at query {}: {}\".format(\"q2\",avg_prec_at_q_2))\n",
    "print(\"Average precision at query {}: {}\".format(\"q3\",avg_prec_at_q_3))\n",
    "print(\"Average precision at query {}: {}\".format(\"q4\",avg_prec_at_q_4))\n",
    "print(\"Average precision at query {}: {}\".format(\"q5\",avg_prec_at_q_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(data, k=10):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_res: search results dataset containing:\n",
    "        q_id: query id.\n",
    "        doc_id: document id.\n",
    "        predicted_relevance: relevance predicted through LightGBM.\n",
    "        y_true: actual score of the document for the query (ground truth).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean average precision @k : float\n",
    "    '''\n",
    "    avp = []\n",
    "    for q in np.unique(data[\"q_id\"]): #loop over all query id\n",
    "        curr_data = data[data[\"q_id\"] == q][\"relevant\"]  # select data for current query\n",
    "        avp.append(avg_precision_at_k(list(curr_data),k)) #append average precision for current query\n",
    "    return  np.sum(avp)/len(avp)  # return mean average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average precision: 0.40777777777777774\n"
     ]
    }
   ],
   "source": [
    "map_k = map_at_k(data, 10)\n",
    "\n",
    "print(\"Mean Average precision: {}\".format(map_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_at_k(y_true, k=10):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: we have already ordered the relevance array\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Reciprocal Rank for qurrent query\n",
    "    '''\n",
    "\n",
    "    if np.sum(y_true) == 0: # if there are not relevant doument return 0\n",
    "        return 0\n",
    "    \n",
    "    if y_true.index(1)+1 > k:\n",
    "        return 0\n",
    "    \n",
    "    return  1/(y_true.index(1)+1) # hint: to get the position of the first relevant document use \"np.argmax\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr = rr_at_k(list(data[data[\"q_id\"]==\"q5\"][\"relevant\"]))\n",
    "\n",
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0.4, 5: 0.4, 10: 0.4}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr = {}\n",
    "for k in [3,5,10]:\n",
    "    RRs = []\n",
    "    for q in np.unique(data[\"q_id\"]): # loop over all query ids\n",
    "        RRs.append(rr_at_k(list(data[data[\"q_id\"]== q][\"relevant\"]),k)) # append RR for current query\n",
    "    mrr[k] = np.round(np.sum(RRs)/len(np.unique(data[\"q_id\"])),4) # Mean RR at current k\n",
    "                      \n",
    "mrr                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berni\\Anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\berni\\Anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "words_count = collections.Counter()\n",
    "for tweet in tweets_keywords.values():\n",
    "    words_count += collections.Counter(tweet[0])\n",
    "\n",
    "model = Word2Vec(words_count)\n",
    "X = model.wv[model.wv.key_to_index]\n",
    "\n",
    "tsne = TSNE(perplexity=5, n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_tsne[:,0], X_tsne[:,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity = {}\n",
    "\n",
    "like_importance = 0.6\n",
    "retweet_importance = 0.4\n",
    "\n",
    "for tweet_id in tweets.keys():\n",
    "    likes = tweets_info[int(tweet_id)]['Likes']\n",
    "    retweets = tweets_info[int(tweet_id)]['Retweets']\n",
    "    \n",
    "    score = (like_importance*likes + retweet_importance*retweets)*0.0001\n",
    "    \n",
    "    popularity[int(tweet_id)] = score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_rank_tweets(terms, tweets, index, idf, tf, popularity):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    tweet_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex]= query_terms_count[term]/query_norm * idf[term]\n",
    "        \n",
    "        # Generate doc_vectors for matching docs\n",
    "        for tweet_index, (tweet, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26  \n",
    "            \n",
    "            \n",
    "            if tweet in tweets:\n",
    "                tweet_vectors[tweet][termIndex] = tf[term][tweet_index] * idf[term] + popularity[int(tweet)]  # TODO: check if multiply for idf\n",
    "\n",
    "                \n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queryVector and each tweetVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "    \n",
    "    tweet_scores=[[np.dot(curTweetVec, query_vector), tweet] for tweet, curTweetVec in tweet_vectors.items() ]\n",
    "    tweet_scores.sort(reverse=True)\n",
    "    result_tweets = [x[1] for x in tweet_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_tweets) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return result_tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_search_tf_idf(query, index, popularity):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)[0]\n",
    "    tweets = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_tweets the ids of the tweets that contain \"term\"                        \n",
    "            term_tweets = []\n",
    "            for posting in index[term]:\n",
    "                term_tweets.append(posting[0])\n",
    "            \n",
    "            # tweets = tweets Union term_tweets\n",
    "            tweets = tweets.union(term_tweets)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    tweets = list(tweets)\n",
    "    ranked_tweets = our_rank_tweets(query, tweets, index, idf_k, tf_k, popularity)\n",
    "    return ranked_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "fast food\n",
      "\n",
      "======================\n",
      "Top 10 results out of 57 for the searched query:\n",
      "======================\n",
      "\n",
      "tweet_id= 1599\n",
      "tweet: It‚Äôs #WorldBreastfeedingWeek \n",
      "\n",
      "Breast milk is the ideal first food for a üë∂. It's safe, clean and contains antibodies which help protect against illnesses.\n",
      "\n",
      "‚úÖ Start ü§± within 1 hour after birth\n",
      "‚úÖ ü§± exclusively for the first 6 months\n",
      "‚úÖ Continue until the üë∂ is 2+ years old https://t.co/CPiaCcEEyk\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1443\n",
      "tweet: ü•µ Heat waves\n",
      "üåä floods\n",
      "üçÇ droughts\n",
      "¬†\n",
      "are taking thousands of lives, forcing displacement, and exacerbating food insecurity, hunger, and malnutrition.\n",
      "¬†\n",
      "#ClimateCrisis¬†is the single biggest health threat facing humanity. https://t.co/AMRkqJ1a4Y\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1753\n",
      "tweet: Unhealthy diets cause 11 million deaths per year.\n",
      "\n",
      "#FoodSystems for Health save lives by:\n",
      "‚ÜòÔ∏ènutritious foods costs\n",
      "‚ÜóÔ∏è availability &amp; affordability of‚ÄØhealthy diets\n",
      "‚úÖEnsuring fair prices that reflect the true costs on environment, health &amp; livelihoods\n",
      "\n",
      "üëâhttps://t.co/0PW0i4rHNW https://t.co/QObwSGxcrE\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 2026\n",
      "tweet: Food systems &amp; health are connected:\n",
      "¬†\n",
      "üîÄ from those who work üë©üèΩ‚Äçüåæ in our food systems\n",
      "üîÄ the practices used to grow, raise, transport package &amp; dispose of our food\n",
      "üîÄ to the quality, quantity &amp; safety of the food we eat\n",
      "¬†\n",
      "More info üëâhttps://t.co/uCqRGjlf3P https://t.co/73YoAlI87v\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 548\n",
      "tweet: Food ü•ïüçÖüçåü•¶ is about more than what we eat.\n",
      "\n",
      "During the UN Food Systems Summit at #UNGA, WHO is committed to transforming #FoodSystems for better health where everyone, everywhere can access healthy &amp; affordable diets.\n",
      "\n",
      "Join us now üëâhttps://t.co/lpc6RTPOJN https://t.co/0A6pKRzjRC\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 883\n",
      "tweet: To keep food safe at home, follow WHO 5 Keys to Safer Food:¬†\n",
      "1Ô∏è‚É£ Keep clean¬†\n",
      "2Ô∏è‚É£ Separate raw and cooked¬†\n",
      "3Ô∏è‚É£ Cook thoroughly¬†\n",
      "4Ô∏è‚É£ Keep food at safe temperature¬†\n",
      "5Ô∏è‚É£ Use safe water and raw materials\n",
      "\n",
      "#SafeFood https://t.co/ppX7O2Jpk7\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 2029\n",
      "tweet: Global hunger, malnutrition &amp; food insecurity have dramatically worsened due to #COVID19.\n",
      "\n",
      "üÜï State of Food Security and Nutrition in the World 2021 report explains why üëâhttps://t.co/8QvHn537QL\n",
      "\n",
      "#SOFI2021 https://t.co/wFOcWqXDxM\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1063\n",
      "tweet: The Global Leaders Group on #AntimicrobialResistance is urging all countries to significantly reduce the levels of antimicrobial drugs used in global food systems and use antimicrobial drugs more responsibly overall https://t.co/W80Ke2HBrY https://t.co/Dklujqw2bk\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 789\n",
      "tweet: Act fast. Seek medical care if you or someone you know has an infection that is not getting better, or if they have any signs of #sepsis.\n",
      "üî∏Fever or shivering\n",
      "üî∏Confusion\n",
      "üî∏Shortness of breath \n",
      "üî∏Rash that does not fade when pressed\n",
      "üî∏Extreme discomfort\n",
      "üî∏Clammy or sweaty skin https://t.co/ZCgfONXZlJ\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 888\n",
      "tweet: More than 600 million people fall ill every year from eating food contaminated with bacteria, viruses, parasites, toxins or chemicals. \n",
      "Everyone has a role to play to keep food safe https://t.co/xPN75h2GdR https://t.co/ozDeAbT8kM\n",
      "\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_tweets = our_search_tf_idf(query, keywords_index, popularity)\n",
    "top = 10\n",
    "\n",
    "relevance_dict[\"q2\"] = {\"tweets_id\": ranked_tweets[:top], \"relevance\": [1,1,0,1,0,0,0,1,0,1,0]} \n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n======================\\n\".format(top, len(ranked_tweets)))\n",
    "for t_id in ranked_tweets[:top]:\n",
    "    print(\"tweet_id= {}\\ntweet: {}\\n\".format(t_id, tweets[str(t_id)]['full_text']))\n",
    "    print(\"--------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "tweets2vec = {}\n",
    "\n",
    "for index in tweets_keywords.keys():\n",
    "    sentences.append(tweets_keywords[index][0])\n",
    "\n",
    "    \n",
    "model = Word2Vec(sentences, workers=4, min_count=1, window=10, sample=1e-3)\n",
    "X = model.wv\n",
    "\n",
    "word2vec = {}\n",
    "for word in X.index_to_key:\n",
    "    word2vec[word] = X[word]\n",
    "\n",
    "for index in tweets_keywords.keys():\n",
    "    words_vecs = []\n",
    "    for word in tweets_keywords[index][0]:\n",
    "        if word in word2vec.keys():\n",
    "            words_vecs.append(word2vec[word])\n",
    "    \n",
    "    tweets2vec[index] = np.mean(words_vecs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_cosine(query):\n",
    "    query = build_terms(query)\n",
    "    \n",
    "    words_vecs_query = []\n",
    "    for word in query[0]:\n",
    "        if word in word2vec.keys():\n",
    "            words_vecs_query.append(word2vec[word])\n",
    "    \n",
    "    query_vector = np.mean(words_vecs_query, axis=0)\n",
    "    \n",
    "    tweet_scores=[[np.dot(curTweetVec, query_vector), tweet] for tweet, curTweetVec in tweets2vec.items() if tweet!=204]\n",
    "    tweet_scores.sort(reverse=True)\n",
    "    \n",
    "    return tweet_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "covid vaccine\n",
      "\n",
      "======================\n",
      "Top 20 results out of 2398 for the searched query:\n",
      "======================\n",
      "\n",
      "tweet_id= 1820\n",
      "tweet: When will the #COVID19 pandemic be over? https://t.co/bebc6ccur1\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 2250\n",
      "tweet: RT @DrTedros: We are calling on #G20 countries to share more #COVID19 vaccine doses now, including by ensuring at least 1 billion doses are‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 909\n",
      "tweet: RT @DrTedros: At the @g20org Health Minister meeting, I called for commitment &amp; support of #G20 countries to reach @WHO's global #COVID19 t‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 2167\n",
      "tweet: Every year harmful use of #alcohol üçª is responsible for 3 million deaths. \n",
      "https://t.co/Eiqv85jbln\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1205\n",
      "tweet: #ClimateChange is a health problem.\n",
      "#ClimateChange is a health problem.\n",
      "#ClimateChange is a health problem.\n",
      "#ClimateChange is a health problem.\n",
      "¬†\n",
      "This #WorldHumanitarianDay, let's take care of our health and our world üåéüåçüåè!\n",
      "\n",
      "https://t.co/yhFXiBAGGE\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 2386\n",
      "tweet: Here is why you should get vaccinated even if you have had #COVID19 üëá\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 627\n",
      "tweet: RT @DrTedros: Health is a service that needs to be continued at all times. If #Afghanistan's health system collapses, the tragedy will be w‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1469\n",
      "tweet: Getting vaccinated üíâ can help protect you and those around you from #COVID19 ‚¨áÔ∏è https://t.co/3GrojOAWT6\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 855\n",
      "tweet: @DrTedros \"We thank those countries that recognize all vaccines with WHO Emergency Use Listing, and we call on all countries to do the same\"-@DrTedros #COVID19 https://t.co/SxVrUSmtqw\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 239\n",
      "tweet: LIVE with @DrTedros at the Global #mentalhealth summit ‚¨áÔ∏è\n",
      "\n",
      "#MindRightsNow \n",
      "\n",
      "https://t.co/M3JiSuUweA\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 873\n",
      "tweet: RT @DrTedros: .@WHO remains committed to working with @_AfricanUnion to end the #COVID19 pandemic, and to support African countries on the‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1493\n",
      "tweet: RT @DrTedros: .@WHO &amp; partners are:\n",
      "1. providing access to life-saving health services for those worst-affected by the crisis\n",
      "2. supporting‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1202\n",
      "tweet: On #WorldHumanitarianDay &amp; every day, let's thank all health care and front-line workers around the world responding to #COVID19, the #ClimateCrisis &amp; other public health emergencies. https://t.co/BTdeGeMbMm\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1317\n",
      "tweet: We stand in solidarity with the people of¬†#Haiti.¬†\n",
      "\n",
      "WHO and¬†@pahowho¬†are working to assess health needs and to support emergency medical care.¬†\n",
      "\n",
      "https://t.co/FQvscKKw4y\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1868\n",
      "tweet: Getting vaccinated üíâ against #COVID19 helps protect you from getting sick. https://t.co/9Ad3uW0NsN\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 774\n",
      "tweet: RT @DrTedros: #Sepsis kills 11 million people each year, including almost 3 million young children. It disables millions more. On #WorldSep‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 657\n",
      "tweet: RT @DrTedros: #Qatar‚Äôs health workforce has been mobilized to provide health care in the compound, including immunization, mental health su‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 81\n",
      "tweet: RT @DrTedros: This #WorldMentalHealthDay, @WHO is also providing new easy-to-read materials on how to take care of your own #mentalhealth a‚Ä¶\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 1645\n",
      "tweet: @DrTedros \"We now have 152 country offices around the world. They are central to what WHO does ‚Äì supporting countries to strengthen health systems and improve the health of their populations\"-@DrTedros #WHOImpact \n",
      "\n",
      "https://t.co/zOxdUJhQHd\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "tweet_id= 437\n",
      "tweet: @DrTedros @EmmanuelMacron @ACTAccelerator \"This https://t.co/RjHSBkVjlH platform provides powerful proof of concept for the #WHOAcademy, which we expect to have even greater reach and greater impact\"-@DrTedros\n",
      "\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_tweets = word2vec_cosine(query)\n",
    "top = 20\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n======================\\n\".format(top, len(ranked_tweets)))\n",
    "for t_id in ranked_tweets[:top]:\n",
    "    print(\"tweet_id= {}\\ntweet: {}\\n\".format(t_id[1], tweets[str(t_id[1])]['full_text']))\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
